{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5050af99-3cad-4ed1-93e6-df3d9e56156d",
   "metadata": {},
   "source": [
    "# DDG - MEDIA BIAS : LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca42e32-419a-4ce2-8da4-cd736f956785",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastapi==0.104.1\n",
    "uvicorn==0.23.2\n",
    "langchain==0.0.354\n",
    "python-multipart==0.0.6\n",
    "llama_cpp_python==0.2.26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7737c1d-ca64-4bfa-80f9-54e0f474170e",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "042fddda-bc2b-40f1-af22-0dcbfff0666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama2\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Prompt\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.prompt_selector import ConditionalPromptSelector\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Output Parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# For using Llama2 with GGUF\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "# For DataFrame\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27c42587-0295-4eb9-b540-2375c9e408af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama-2-7b-chat_Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   123.81 MiB, ( 1097.42 / 12288.02)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "llm_load_tensors:      Metal buffer size =   123.81 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Pro\n",
      "ggml_metal_init: picking default device: Apple M3 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/abdulrafay/Library/Python/3.9/lib/python/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M3 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 12884.92 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   992.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    32.00 MiB, ( 1130.42 / 12288.02)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    32.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    28.03 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   328.02 MiB, ( 1458.44 / 12288.02)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   328.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   336.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 3\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "model_path = \"llama-2-7b-chat_Q4_K_M.gguf\"\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=1,\n",
    "    n_batch=1024,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"Cleaned_2020_articles_mass.csv\")\n",
    "\n",
    "# is it in Boston? Y/N -> first character\n",
    "# tell me the LOCATION where this article took place, SPECIFIC AS POSSIBLE\n",
    "# your response may not exceed 2045 words.\n",
    "\n",
    "prompt = PromptTemplate( input_variables=[\"headline\", \"body\"], template=\"\"\"<<SYS>> \\n <</SYS>> \\n\\n [INST] \n",
    "    Generate response : is it in boston? yes or no,\n",
    "    tell me the LOCATION where this article took place, BE SPECIFIC AS POSSIBLE,\n",
    "    your response may not exceed 2045 words.\"\\n\\n\n",
    "    Headline: \\n\\n {headline} \\n\\n Body: {body} \\n\\n [/INST]\"\"\",\n",
    "    )\n",
    "\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f88e1ba5-8e50-4527-8d76-c731828dfa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder function for generating summaries - replace this with your actual model interaction\n",
    "\n",
    "def generate_summary(headline, body):\n",
    "\n",
    "    summary = chain.invoke({\n",
    "    \"headline\": headline,\n",
    "    \"body\": body\n",
    "    })\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e93210b-2338-43d2-aa9d-297eb310dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sliced = df.iloc[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7fe0f72-7b66-484a-9d27-41898ecd4022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count < 2045 -> 100 rows\n",
    "df_filtered = df.loc[df['actual_body_count'] < 2045].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14e74e67-26b4-4c79-81cb-2f37bdcd4700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Here is a short sentence for the given headline and body:\n",
      "Boston has a 3-10 record in its last 13 games against Central teams, including a 6-4 loss to Chicago in their last matchup."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   18509.63 ms\n",
      "llama_print_timings:      sample time =       3.32 ms /    52 runs   (    0.06 ms per token, 15643.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17672.37 ms /  1071 tokens (   16.50 ms per token,    60.60 tokens per second)\n",
      "llama_print_timings:        eval time =    2578.27 ms /    51 runs   (   50.55 ms per token,    19.78 tokens per second)\n",
      "llama_print_timings:       total time =   20340.46 ms /  1122 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Here is a short sentence based on the provided headline and body:\n",
      "The state funding formula is too generous to wealthy schools, according to a new report from the Business Alliance, which found that nearly half of all state aid goes to the wealthiest 20% of districts despite the need for increased funding in lower-income areas, where gaps in opportunities for achievement are likely to widen due to recent school closures related to the COVID-19 pandemic."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   18509.63 ms\n",
      "llama_print_timings:      sample time =       6.79 ms /   105 runs   (    0.06 ms per token, 15463.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10260.04 ms /   697 tokens (   14.72 ms per token,    67.93 tokens per second)\n",
      "llama_print_timings:        eval time =    5212.35 ms /   104 runs   (   50.12 ms per token,    19.95 tokens per second)\n",
      "llama_print_timings:       total time =   15649.10 ms /   801 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here is a short sentence summarizing the information in the given headline and body:\n",
      "In Massachusetts, there have been over 9,000 COVID-19 deaths, with a large party involving 50-60 Lincoln Sudbury Regional High School students breaking out on Saturday night, prompting officials to delay the return of students to school due to the potential impacts of the gathering, despite students wearing masks and practicing social distancing."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   18509.63 ms\n",
      "llama_print_timings:      sample time =       6.33 ms /   101 runs   (    0.06 ms per token, 15968.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5292.53 ms /   345 tokens (   15.34 ms per token,    65.19 tokens per second)\n",
      "llama_print_timings:        eval time =    4669.38 ms /   100 runs   (   46.69 ms per token,    21.42 tokens per second)\n",
      "llama_print_timings:       total time =   10127.56 ms /   445 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here's a short sentence summarizing the article:\n",
      "Rookie Aaron Nesmith had a pivotal performance in Game 7 of the Eastern Conference finals, hitting clutch free throws and deflecting a potential game-tying shot to help the Celtics defeat the Raptors."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   18509.63 ms\n",
      "llama_print_timings:      sample time =       4.55 ms /    70 runs   (    0.06 ms per token, 15388.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10196.68 ms /   698 tokens (   14.61 ms per token,    68.45 tokens per second)\n",
      "llama_print_timings:        eval time =    3511.68 ms /    69 runs   (   50.89 ms per token,    19.65 tokens per second)\n",
      "llama_print_timings:       total time =   13823.62 ms /   767 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  In Boston, college students are facing harsh penalties for breaking COVID-19 rules, while conservatives argue that they are not being held accountable like other adults."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   18509.63 ms\n",
      "llama_print_timings:      sample time =       2.54 ms /    39 runs   (    0.07 ms per token, 15372.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4552.02 ms /   274 tokens (   16.61 ms per token,    60.19 tokens per second)\n",
      "llama_print_timings:        eval time =    1824.35 ms /    38 runs   (   48.01 ms per token,    20.83 tokens per second)\n",
      "llama_print_timings:       total time =    6440.42 ms /   312 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here is a short sentence summarizing the information in the headline and body:\n",
      "In Clinton, Massachusetts, an intrepid group of visitors can marvel at two engineering marvels - the 1906 Wachusett Dam and the Wachusett Reservoir - which were constructed using labor from local workers who worked year-round for five years to build the 944-foot-long dam that creates water for nearly 3 million people in the Boston area."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   18509.63 ms\n",
      "llama_print_timings:      sample time =       9.03 ms /   107 runs   (    0.08 ms per token, 11850.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4601.23 ms /   282 tokens (   16.32 ms per token,    61.29 tokens per second)\n",
      "llama_print_timings:        eval time =    5007.59 ms /   106 runs   (   47.24 ms per token,    21.17 tokens per second)\n",
      "llama_print_timings:       total time =    9785.55 ms /   388 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The New England Patriots explored trading defensive lineman Michael Gilmore before last season."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   18509.63 ms\n",
      "llama_print_timings:      sample time =       1.41 ms /    22 runs   (    0.06 ms per token, 15613.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3173.47 ms /   157 tokens (   20.21 ms per token,    49.47 tokens per second)\n",
      "llama_print_timings:        eval time =     896.15 ms /    21 runs   (   42.67 ms per token,    23.43 tokens per second)\n",
      "llama_print_timings:       total time =    4103.11 ms /   178 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  In 1898, the Supreme Court issued a decision in Williams v. Mississippi that denied many African Americans in the South the right to vote, setting off a long struggle for voting rights that would take decades to overcome."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   18509.63 ms\n",
      "llama_print_timings:      sample time =       4.13 ms /    49 runs   (    0.08 ms per token, 11850.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5650.33 ms /   374 tokens (   15.11 ms per token,    66.19 tokens per second)\n",
      "llama_print_timings:        eval time =    2366.74 ms /    48 runs   (   49.31 ms per token,    20.28 tokens per second)\n",
      "llama_print_timings:       total time =    8098.05 ms /   422 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  States slow to share warnings on truckers; delays cost lives.\n",
      "In the United States, state motor vehicle agencies are struggling to send timely warnings to trucking companies about dangerous drivers, leading to delays that have cost lives. According to recent reports, only 24% of state agencies sent out conviction notices within the required 10-day window, while 70% failed to do so within the federally mandated time frame. These delays are despite efforts by federal authorities to address the issue, including providing $370 million in grant funding to help states improve their commercial driving safety improvements. Experts say that the delays are unacceptable and are putting lives at risk, as truckers are often operating within just three to four feet of other vehicles on interstate highways."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   18509.63 ms\n",
      "llama_print_timings:      sample time =      11.13 ms /   176 runs   (    0.06 ms per token, 15817.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19875.34 ms /  1231 tokens (   16.15 ms per token,    61.94 tokens per second)\n",
      "llama_print_timings:        eval time =    9168.35 ms /   175 runs   (   52.39 ms per token,    19.09 tokens per second)\n",
      "llama_print_timings:       total time =   29353.40 ms /  1406 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  In Massachusetts, there is a fundamental racial bias in the court system, with mostly white judges and a lack of diversity on the bench, according to a recent study."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   18509.63 ms\n",
      "llama_print_timings:      sample time =       2.52 ms /    39 runs   (    0.06 ms per token, 15445.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6845.98 ms /   455 tokens (   15.05 ms per token,    66.46 tokens per second)\n",
      "llama_print_timings:        eval time =    1843.68 ms /    38 runs   (   48.52 ms per token,    20.61 tokens per second)\n",
      "llama_print_timings:       total time =    8754.03 ms /   493 tokens\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the DataFrame and generate summaries\n",
    "#df['Summary'] = df_filtered.apply(lambda row: generate_summary(row['hl1'], row['body_cleaned']), axis=1)\n",
    "df_filtered['Summary'] = df_filtered.apply(lambda row: generate_summary(row['hl1'], row['body_cleaned']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21681467-5834-4885-b9b1-fa3555cca4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit number of rows to iterate over\n",
    "#row_limit = 1\n",
    "\n",
    "# Slice the DataFrame to the first 'row_limit' rows and then apply the function\n",
    "#df = df.loc[:row_limit-1, 'Summary'] = df.loc[:row_limit-1].apply(lambda row: generate_summary(row['hl1'], row['body_cleaned']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203afde5-cd50-4fb5-bba4-62abddff8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8908e29e-f31b-4bab-88c5-25cc90c44620",
   "metadata": {},
   "source": [
    "# Export as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fcaf6a-fef7-4efc-b134-e43ee8c698a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv(\"2020_articles_mass_summaries_10rows.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f364e0cf-3dd8-4ec8-ad00-6eab1cb4aa46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
